{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def initialize_p(p1,p2):\n",
    "    p = [[[0 for s in range(15)] for a in range(4)] for sp in range(15)]\n",
    "    # p[s][a][s']\n",
    "    # 0 = up, 1 = right, 2 = down, 3 = left\n",
    "\n",
    "    # for squares where there is no chance of moving off the edge\n",
    "    for s in [5,6,9,10,13,14]:\n",
    "        p[s][0][s-4] = p1\n",
    "        p[s][0][s] = p2\n",
    "        p[s][0][s-3] = (1-p1-p2)/2\n",
    "        p[s][0][s-5] = (1-p1-p2)/2\n",
    "\n",
    "    for s in [4,5,6,8,9,10]:\n",
    "        if s == 10:\n",
    "            p[s][1][s+1] = p1\n",
    "            p[s][1][s] = p2\n",
    "            p[s][1][7] = (1-p1-p2)/2\n",
    "            p[s][1][0] = (1-p1-p2)/2\n",
    "        else:\n",
    "            p[s][1][s+1] = p1\n",
    "            p[s][1][s] = p2\n",
    "            p[s][1][s-3] = (1-p1-p2)/2\n",
    "            p[s][1][s+5] = (1-p1-p2)/2\n",
    "\n",
    "    for s in [1,2,5,6,9,10]:\n",
    "        if s == 10:\n",
    "            p[s][2][s+4] = p1\n",
    "            p[s][2][s] = p2\n",
    "            p[s][1][13] = (1-p1-p2)/2\n",
    "            p[s][1][0] = (1-p1-p2)/2\n",
    "        else:\n",
    "            p[s][2][s+4] = p1\n",
    "            p[s][2][s] = p2\n",
    "            p[s][2][s+3] = (1-p1-p2)/2\n",
    "            p[s][2][s+5] = (1-p1-p2)/2\n",
    "\n",
    "    for s in [5,6,7,9,10,11]:\n",
    "        p[s][3][s-1] = p1\n",
    "        p[s][3][s] = p2\n",
    "        p[s][3][s+3] = (1-p1-p2)/2\n",
    "        p[s][3][s-5] = (1-p1-p2)/2\n",
    "\n",
    "    # for squares where there is a chance of moving to an adjacent square which is off the edge\n",
    "    for s in [4,7,8,11,12]:\n",
    "        p[s][0][s-4] = p1 + (1-p1-p2)/2\n",
    "        p[s][0][s] = p2\n",
    "        if s in [4,8,12]:\n",
    "            p[s][0][s-3] = (1-p1-p2)/2\n",
    "        else:\n",
    "            p[s][0][s-5] = (1-p1-p2)/2\n",
    "\n",
    "    for s in [1,2,12,13,14]:\n",
    "        if s == 14:\n",
    "            p[s][1][0] = p1 + (1-p1-p2)/2\n",
    "            p[s][1][s] = p2\n",
    "            p[s][1][11] = (1-p1-p2)/2\n",
    "        else:\n",
    "            p[s][1][s+1] = p1 + (1-p1-p2)/2\n",
    "            p[s][1][s] = p2\n",
    "            if s in [12,13]:\n",
    "                p[s][1][s-3] = (1-p1-p2)/2\n",
    "            else:\n",
    "                p[s][1][s+5] = (1-p1-p2)/2\n",
    "\n",
    "    for s in [3,4,7,8,11]:\n",
    "        if s == 11:\n",
    "            p[s][2][0] = p1 + (1-p1-p2)/2\n",
    "            p[s][2][s] = p2\n",
    "            p[s][2][14] = (1-p1-p2)/2\n",
    "        else:\n",
    "            p[s][2][s+4] = p1 + (1-p1-p2)/2\n",
    "            p[s][2][s] = p2\n",
    "            if s in [3,7,11]:\n",
    "                p[s][0][s+3] = (1-p1-p2)/2\n",
    "            else:\n",
    "                p[s][0][s+5] = (1-p1-p2)/2\n",
    "\n",
    "    for s in [1,2,3,13,14]:\n",
    "        p[s][3][s-1] = p1 + (1-p1-p2)/2\n",
    "        p[s][3][s] = p2\n",
    "        if s in [1,2,3]:\n",
    "            p[s][1][s+3] = (1-p1-p2)/2\n",
    "        else:\n",
    "            p[s][1][s-5] = (1-p1-p2)/2\n",
    "\n",
    "    # for squares where the action will take agent of the edge\n",
    "    for s in [1,2,3]:\n",
    "        p[s][0][s] = p1+p2\n",
    "    p[1][0][0] = (1-p1-p2)/2\n",
    "    p[1][0][2] = (1-p1-p2)/2\n",
    "    p[2][0][1] = (1-p1-p2)/2\n",
    "    p[2][0][3] = (1-p1-p2)/2\n",
    "    p[3][0][2] = (1-p1-p2)/2\n",
    "    p[3][0][7] = (1-p1-p2)/2\n",
    "\n",
    "    for s in [3,7,11]:\n",
    "        p[s][1][s] = p1+p2\n",
    "    p[3][1][2] = (1-p1-p2)/2\n",
    "    p[3][1][7] = (1-p1-p2)/2\n",
    "    p[7][1][3] = (1-p1-p2)/2\n",
    "    p[7][1][11] = (1-p1-p2)/2\n",
    "    p[11][1][7] = (1-p1-p2)/2\n",
    "    p[11][1][0] = (1-p1-p2)/2\n",
    "\n",
    "    for s in [12,13,14]:\n",
    "        p[s][2][s] = p1+p2\n",
    "    p[12][2][8] = (1-p1-p2)/2\n",
    "    p[12][2][13] = (1-p1-p2)/2\n",
    "    p[13][2][12] = (1-p1-p2)/2\n",
    "    p[13][2][14] = (1-p1-p2)/2\n",
    "    p[14][2][13] = (1-p1-p2)/2\n",
    "    p[14][2][0] = (1-p1-p2)/2\n",
    "\n",
    "    for s in [4,8,12]:\n",
    "        p[s][3][s] = p1+p2\n",
    "    p[4][3][0] = (1-p1-p2)/2\n",
    "    p[4][3][8] = (1-p1-p2)/2\n",
    "    p[8][3][4] = (1-p1-p2)/2\n",
    "    p[8][3][12] = (1-p1-p2)/2\n",
    "    p[12][3][8] = (1-p1-p2)/2\n",
    "    p[12][3][13] = (1-p1-p2)/2\n",
    "\n",
    "    return p\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_PRvalues():\n",
    "    p1 = float(input(\"Enter p1 <= 1: \"))\n",
    "    p2 = float(input(\"Enter p2 <= 1 - p1: \"))\n",
    "    r0 = float(input(\"Enter reward for action Up: \"))\n",
    "    r1 = float(input(\"Enter reward for action Right: \"))\n",
    "    r2 = float(input(\"Enter reward for action Down: \"))\n",
    "    r3 = float(input(\"Enter reward for action Left: \"))\n",
    "    return p1, p2, r0, r1, r2, r3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyIteration:\n",
    "    def __init__(self,p1=1,p2=0, r0=-1,r1=-1,r2=-1,r3=-1):\n",
    "        self.A = [0,1,2,3] # 0 = up, 1 = right, 2 = down, 3 = left\n",
    "        self.R = [r0, r1, r2, r3]\n",
    "        self.p = initialize_p(p1,p2)\n",
    "        self.S = [1,2,3,4,5,6,7,8,9,10,11,12,13,14]\n",
    "        self.theta = 0.001\n",
    "        self.gamma = 0.95\n",
    "        self.V = []\n",
    "        self.pi = []\n",
    "\n",
    "    # initializes value and policy arrays\n",
    "    def Initialization(self):\n",
    "        self.V = [0]*15\n",
    "                \n",
    "        self.pi = []\n",
    "        for i in range(0,14):\n",
    "            self.pi.append(random.choices(self.A, weights = [0.25,0.25,0.25,0.25])[0])\n",
    "\n",
    "    def Policy_Evaluation(self):\n",
    "        #set delta to enter while loop\n",
    "        delta = self.theta\n",
    "        \n",
    "        # iterate through every state and calculate the value for that state\n",
    "        # each iteration is a closer approximation to the true value\n",
    "        # stop iterating when the value updates are less than the theta threshold (accuracy parameter)\n",
    "        while delta >= self.theta:\n",
    "            delta = 0\n",
    "            for s in self.S:\n",
    "                v = self.V[s] # store previous value\n",
    "                summation = 0\n",
    "                for s_prime in range(0,15):\n",
    "                    summation += self.p[s][self.pi[s-1]][s_prime] * (self.R[self.pi[s-1]] + self.gamma * self.V[s_prime])\n",
    "                self.V[s] = round(summation,3)\n",
    "                delta = max(delta, abs(v - self.V[s]))\n",
    "\n",
    "    def Policy_Improvement(self):\n",
    "        policy_stable = True\n",
    "        \n",
    "        # iterate through each state and calculate the value for each action in each state\n",
    "        # if an action has a higher value than the action currently in the policy, update the policy & value array with the new action & value\n",
    "        # after iterating through every state, if there has been no changes to the policy, optimal policy has been found\n",
    "        for s in self.S:\n",
    "            old_action = self.pi[s-1]\n",
    "            for a in range(0,4):\n",
    "                summation = 0\n",
    "                for s_prime in range(0,15):\n",
    "                    summation += self.p[s][a][s_prime] * (self.R[a] + self.gamma * self.V[s_prime])\n",
    "                if summation > self.V[s]:\n",
    "                    self.pi[s-1] = a\n",
    "                    self.V[s] = summation\n",
    "                if old_action != self.pi[s-1]:\n",
    "                    policy_stable = False\n",
    "        return policy_stable\n",
    "    \n",
    "    def Policy_Iteration(self):\n",
    "        #start counter to count how many times the loop executes\n",
    "        iterations = 1\n",
    "\n",
    "        #initialize value and pi arrays\n",
    "        self.Initialization()\n",
    "\n",
    "        policy_stable = False\n",
    "        while policy_stable == False:\n",
    "            #get the start time at the beginning of each loop\n",
    "            start_time = time.time()\n",
    "\n",
    "            self.Policy_Evaluation()\n",
    "            policy_stable = self.Policy_Improvement()\n",
    "            \n",
    "            print(\"Elapsed time for iteration\", iterations, \":\", time.time() - start_time)\n",
    "            print(f\"Value function for iteration {iterations}: {self.V}\")\n",
    "            iterations += 1\n",
    "        \n",
    "        print(\"Optimized policy: \", self.pi)\n",
    "        print(\"Optimized value function: \", self.V)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time for iteration 1 : 0.015990495681762695\n",
      "Value function for iteration 1: [0, -1.0, -1.95, -2.8525, -1.0, -1.95, -2.8525, -3.709875, -1.95, -2.8525, -3.709875, -1.0, -2.8525, -3.709875, -1.0]\n",
      "Elapsed time for iteration 2 : 0.0010135173797607422\n",
      "Value function for iteration 2: [0, -1.0, -1.95, -2.8525, -1.0, -1.95, -2.8525, -1.95, -1.95, -2.8525, -1.95, -1.0, -2.8525, -1.95, -1.0]\n",
      "Elapsed time for iteration 3 : 0.0009999275207519531\n",
      "Value function for iteration 3: [0, -1.0, -1.95, -2.8525, -1.0, -1.95, -2.8525, -1.95, -1.95, -2.8525, -1.95, -1.0, -2.8525, -1.95, -1.0]\n",
      "Elapsed time for iteration 4 : 0.0\n",
      "Value function for iteration 4: [0, -1.0, -1.95, -2.8525, -1.0, -1.95, -2.8525, -1.95, -1.95, -2.8525, -1.95, -1.0, -2.8525, -1.95, -1.0]\n",
      "Optimized policy:  [3, 3, 2, 0, 0, 0, 2, 0, 0, 1, 2, 0, 1, 1]\n",
      "Optimized value function:  [0, -1.0, -1.95, -2.8525, -1.0, -1.95, -2.8525, -1.95, -1.95, -2.8525, -1.95, -1.0, -2.8525, -1.95, -1.0]\n"
     ]
    }
   ],
   "source": [
    "p1, p2, r0, r1, r2, r3 = get_PRvalues()\n",
    "\n",
    "pol = PolicyIteration(p1, p2, r0, r1, r2, r3)\n",
    "pol.Policy_Iteration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5e820f3ddc028a719ffe50e7d80dd01658ce1fe998d4f6f388d9b09d11d3d164"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
